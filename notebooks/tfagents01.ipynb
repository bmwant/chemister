{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import dqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn import q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import trajectory\n",
    "from tf_agents.metrics import metric_utils, tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "env_name = 'CartPole-v0'\n",
    "num_iterations = 20000\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_capacity = 100000\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "log_interval = 200\n",
    "num_eval_episodes = 10\n",
    "eval_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([ 0.03654184,  0.02037561, -0.001049  , -0.03980718], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec().observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.02990607, -0.03645543,  0.03585297,  0.02890254], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_time_step = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(1, dtype=int32), reward=array(1., dtype=float32), discount=array(1., dtype=float32), observation=array([-0.03063518,  0.15813452,  0.03643103, -0.25225627], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    train_step_counter=train_step_counter,\n",
    "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
    ")\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TFAgent.policy of <tf_agents.agents.dqn.dqn_agent.DqnAgent object at 0x140806630>>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TFAgent.collect_policy of <tf_agents.agents.dqn.dqn_agent.DqnAgent object at 0x140806630>>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                               train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):    total_return = 0.0 \n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    replay_buffer.add_batch(traj)\n",
    "    \n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2\n",
    ").prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 21.531078338623047\n",
      "step = 400: loss = 56.50679397583008\n",
      "step = 600: loss = 35.09357452392578\n",
      "step = 800: loss = 32.73316192626953\n",
      "step = 1000: loss = 34.38996887207031\n",
      "step = 1000: Average Return = 198.39999389648438\n",
      "step = 1200: loss = 82.58395385742188\n",
      "step = 1400: loss = 132.54940795898438\n",
      "step = 1600: loss = 124.92987823486328\n",
      "step = 1800: loss = 47.248104095458984\n",
      "step = 2000: loss = 30.976490020751953\n",
      "step = 2000: Average Return = 59.29999923706055\n",
      "step = 2200: loss = 336.6656188964844\n",
      "step = 2400: loss = 105.05303955078125\n",
      "step = 2600: loss = 30.639545440673828\n",
      "step = 2800: loss = 99.55473327636719\n",
      "step = 3000: loss = 602.5145263671875\n",
      "step = 3000: Average Return = 61.400001525878906\n",
      "step = 3200: loss = 32.09312438964844\n",
      "step = 3400: loss = 646.1357421875\n",
      "step = 3600: loss = 47.03107833862305\n",
      "step = 3800: loss = 961.888671875\n",
      "step = 4000: loss = 95.11054992675781\n",
      "step = 4000: Average Return = 57.20000076293945\n",
      "step = 4200: loss = 362.79473876953125\n",
      "step = 4400: loss = 140.62449645996094\n",
      "step = 4600: loss = 905.238037109375\n",
      "step = 4800: loss = 791.81201171875\n",
      "step = 5000: loss = 181.74420166015625\n",
      "step = 5000: Average Return = 130.3000030517578\n",
      "step = 5200: loss = 497.1757507324219\n",
      "step = 5400: loss = 24.086318969726562\n",
      "step = 5600: loss = 269.3563232421875\n",
      "step = 5800: loss = 1143.132080078125\n",
      "step = 6000: loss = 62.71086120605469\n",
      "step = 6000: Average Return = 67.0\n",
      "step = 6200: loss = 70.08395385742188\n",
      "step = 6400: loss = 363.80535888671875\n",
      "step = 6600: loss = 241.85345458984375\n",
      "step = 6800: loss = 24.111295700073242\n",
      "step = 7000: loss = 344.10174560546875\n",
      "step = 7000: Average Return = 141.0\n",
      "step = 7200: loss = 38.8997802734375\n",
      "step = 7400: loss = 99.18739318847656\n",
      "step = 7600: loss = 17.61404037475586\n",
      "step = 7800: loss = 600.5834350585938\n",
      "step = 8000: loss = 414.26959228515625\n",
      "step = 8000: Average Return = 157.0\n",
      "step = 8200: loss = 20.02684783935547\n",
      "step = 8400: loss = 14.123218536376953\n",
      "step = 8600: loss = 28.89373016357422\n",
      "step = 8800: loss = 80.52288055419922\n",
      "step = 9000: loss = 419.2340087890625\n",
      "step = 9000: Average Return = 193.1999969482422\n",
      "step = 9200: loss = 256.67510986328125\n",
      "step = 9400: loss = 14.947534561157227\n",
      "step = 9600: loss = 395.34228515625\n",
      "step = 9800: loss = 589.22509765625\n",
      "step = 10000: loss = 13.257735252380371\n",
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 407.9547424316406\n",
      "step = 10400: loss = 17.292442321777344\n",
      "step = 10600: loss = 324.16363525390625\n",
      "step = 10800: loss = 1717.2098388671875\n",
      "step = 11000: loss = 49.145320892333984\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 514.08447265625\n",
      "step = 11400: loss = 63.01993179321289\n",
      "step = 11600: loss = 1021.0872802734375\n",
      "step = 11800: loss = 964.418701171875\n",
      "step = 12000: loss = 37.91742706298828\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 1242.521240234375\n",
      "step = 12400: loss = 1746.5631103515625\n",
      "step = 12600: loss = 248.897216796875\n",
      "step = 12800: loss = 151.05337524414062\n",
      "step = 13000: loss = 1101.4449462890625\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 438.28411865234375\n",
      "step = 13400: loss = 248.16860961914062\n",
      "step = 13600: loss = 29.330703735351562\n",
      "step = 13800: loss = 730.8961181640625\n",
      "step = 14000: loss = 45.49745178222656\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 3136.896728515625\n",
      "step = 14400: loss = 1149.354736328125\n",
      "step = 14600: loss = 1478.6949462890625\n",
      "step = 14800: loss = 749.9140625\n",
      "step = 15000: loss = 651.131591796875\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 72.52986145019531\n",
      "step = 15400: loss = 71.09382629394531\n",
      "step = 15600: loss = 53.930023193359375\n",
      "step = 15800: loss = 67.3087158203125\n",
      "step = 16000: loss = 107.1379165649414\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 2509.5419921875\n",
      "step = 16400: loss = 1555.0823974609375\n",
      "step = 16600: loss = 1803.7117919921875\n",
      "step = 16800: loss = 56.22900390625\n",
      "step = 17000: loss = 4349.1904296875\n",
      "step = 17000: Average Return = 200.0\n",
      "step = 17200: loss = 2455.945556640625\n",
      "step = 17400: loss = 135.10093688964844\n",
      "step = 17600: loss = 3655.994384765625\n",
      "step = 17800: loss = 110.10887145996094\n",
      "step = 18000: loss = 166.3095703125\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 58.304779052734375\n",
      "step = 18400: loss = 71.43017578125\n",
      "step = 18600: loss = 74.11643981933594\n",
      "step = 18800: loss = 5936.5693359375\n",
      "step = 19000: loss = 110.1064453125\n",
      "step = 19000: Average Return = 200.0\n",
      "step = 19200: loss = 426.91748046875\n",
      "step = 19400: loss = 989.296142578125\n",
      "step = 19600: loss = 144.06837463378906\n",
      "step = 19800: loss = 88.01036071777344\n",
      "step = 20000: loss = 126.41265106201172\n",
      "step = 20000: Average Return = 200.0\n",
      "CPU times: user 2min 53s, sys: 7.51 s, total: 3min\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy)\n",
    "        \n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience)\n",
    "    \n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fdc0f5551d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average Return'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'returns' is not defined"
     ]
    }
   ],
   "source": [
    "steps = range(0, num_iterations+1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
